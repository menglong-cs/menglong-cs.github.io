<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>NLP八股文</title>
    <link href="/2024/05/23/NLP%E5%85%AB%E8%82%A1%E6%96%87/"/>
    <url>/2024/05/23/NLP%E5%85%AB%E8%82%A1%E6%96%87/</url>
    
    <content type="html"><![CDATA[<h2 id="1-机器学习篇"><a href="#1-机器学习篇" class="headerlink" title="1.机器学习篇"></a>1.机器学习篇</h2><h2 id="2-RNN-LSTM-GRU"><a href="#2-RNN-LSTM-GRU" class="headerlink" title="2.RNN &amp; LSTM &amp; GRU"></a>2.RNN &amp; LSTM &amp; GRU</h2><h2 id="3-Transformer-GPT-BERT"><a href="#3-Transformer-GPT-BERT" class="headerlink" title="3.Transformer &amp; GPT &amp; BERT"></a>3.Transformer &amp; GPT &amp; BERT</h2><h3 id="3-1-Pre-Norm和Post-Norm的区别"><a href="#3-1-Pre-Norm和Post-Norm的区别" class="headerlink" title="3.1 Pre Norm和Post Norm的区别"></a>3.1 Pre Norm和Post Norm的区别</h3><p>PreNorm: X<del>t+1</del> &#x3D; X<del>t</del> + F(Norm(X<del>t</del>)) &#x3D; X<del>t-1</del> + F(Norm(X<del>t-1</del>)) + F(Norm(X<del>t</del>)) &#x3D; X<del>0</del> + F(Norm(X<del>0</del>)) + … + F(Norm(X<del>t</del>))</p><p>PostNorm: X(t+1) &#x3D; Norm(Xt + F(Xt))</p><p>post-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好</p><p>pre-norm相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模型的梯度爆炸或者梯度消失，训练更稳定</p><p>Post-norm: 训练不稳定，但若成功收敛，性能比pre-norm好</p><p>Pre-norm：训练更稳定，但理论层数不如Post-norm</p><p>层数少选post-norm，层数多选pre-norm</p><h3 id="3-2-self-attention为什么要除根号下d-k"><a href="#3-2-self-attention为什么要除根号下d-k" class="headerlink" title="3.2 self-attention为什么要除根号下d_k"></a>3.2 self-attention为什么要除根号下d_k</h3><p>attention(Q, K, V) &#x3D; softmax(QK^T^&#x2F;$\sqrt(d)$)V</p><ul><li>防止QK^T^的值过大，若QK的值过大，那么经过softmax后</li><li>使得QK^T^的结果满足期望为0，方差为1的分布，类似于归一化</li></ul><p><img src="https://pic4.zhimg.com/80/v2-104261e6ef3093cb2a1aef545e147827_1440w.webp" alt="img" style="zoom: 50%;" /><img src="https://pic4.zhimg.com/80/v2-4821aba4e13085e36c4b0ccd44110b57_1440w.webp" alt="img" style="zoom:50%;" /></p><h3 id="3-3-self-attn的计算复杂度"><a href="#3-3-self-attn的计算复杂度" class="headerlink" title="3.3 self-attn的计算复杂度"></a>3.3 self-attn的计算复杂度</h3><p>计算复杂度为O(n^2 * d)  n是序列长度，d是每个token的维度</p><p>Q * K^T^ &#x3D; [n, d] * [d, n] &#x3D; [n, n] (复杂度为O(n^2^ * d))</p><p>(Q * K^T^) * V &#x3D; [n, n] * [n, d] &#x3D; [n, d] (复杂度为 O(n^2^ * d))</p><h2 id="4-激活函数"><a href="#4-激活函数" class="headerlink" title="4.激活函数"></a>4.激活函数</h2><h3 id="为什么使用激活函数"><a href="#为什么使用激活函数" class="headerlink" title="为什么使用激活函数"></a>为什么使用激活函数</h3><p>因为神经网络中每一层都是线性求和的过程，如果没有激活函数，那么神经网络只能拟合线性映射，无法解决更复杂的问题。加入了激活函数，就给神经网络引入了非线性元素，从而使得神经网络能够拟合任何非线性映射，从而解决更复杂的问题。</p><h3 id="Sigmoid激活函数"><a href="#Sigmoid激活函数" class="headerlink" title="Sigmoid激活函数"></a>Sigmoid激活函数</h3><p>f(x) &#x3D; sigmoid(x) &#x3D; 1&#x2F;(1 + e^-x^)</p><img src="NLP八股文.assets/image-20240523224142848-6475307.png" alt="image-20240523224142848" style="zoom:50%;" /><ul><li>输出值在(0, 1)之间，相当于对每个输出进行了归一化</li><li>可以用于预测概率作为输出的模型</li><li>函数是可微的</li></ul><p>缺点：</p><ul><li>梯度消失：当Sigmod的输入（神经元的输出）较大或较小，梯度接近于0，导致该神经元无法得到更新</li><li>计算成本高</li><li>输出不以0为中心，输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢。</li></ul><h3 id="Tanh-激活函数"><a href="#Tanh-激活函数" class="headerlink" title="Tanh 激活函数"></a>Tanh 激活函数</h3><img src="NLP八股文.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5qCL5qyh5aSn5qyh,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="tanh" style="zoom:50%;" /><p>f(x) &#x3D; tanh(x) &#x3D; (e^x^ - e^-x^) &#x2F; (e^x^ + e^-x^) &#x3D; 2 &#x2F; (1 + e^-2x^) - 1 &#x3D; 2 * sigmoid(2x) - 1</p><ul><li>输出值在(-1,1)之间，输出以0为中心</li></ul><p>缺点：</p><ul><li>梯度消失，当输入较大或较小时，梯度解决于0，导致神经元无法得到更新</li><li>计算成本高</li></ul><h3 id="Softmax激活函数"><a href="#Softmax激活函数" class="headerlink" title="Softmax激活函数"></a>Softmax激活函数</h3><p>Softmax 是用于多类分类问题的激活函数。</p><p>Softmax(x) &#x3D; x<del>i</del> &#x2F; (x<del>0</del> + … + x<del>n</del>) </p><p>缺点：</p><ul><li>在零点不可导</li><li>负输入的梯度为0，因此会产生永不激活的死亡神经元。</li></ul><h3 id="Relu-激活函数"><a href="#Relu-激活函数" class="headerlink" title="Relu 激活函数"></a>Relu 激活函数</h3><img src="NLP八股文.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5qCL5qyh5aSn5qyh,size_20,color_FFFFFF,t_70,g_se,x_16-20240523224224762.png" alt="relu1" style="zoom:50%;" /><p>f(x) &#x3D; relu(x) &#x3D; max(0, x)</p><ul><li>Rleu是较为流行的激活函数</li><li>当输入为正时，梯度为1，一定程度上改善了梯度消失问题，加速梯度下降的收敛速度</li><li>计算成本低</li><li>符合生物学，具有单侧抑制、宽兴奋边界（兴奋程度非常高）</li></ul><p>缺点：</p><ul><li>当输入为负时，梯度为0。若参数有一次不恰当的更新，那么第一个隐藏层中的某个ReLU 神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活。</li></ul><h3 id="Leaky-Relu-激活函数"><a href="#Leaky-Relu-激活函数" class="headerlink" title="Leaky Relu 激活函数"></a>Leaky Relu 激活函数</h3><p>leaky_relu(x) &#x3D; $$ f(x)&#x3D;\left{ \begin{aligned} &amp;x, &amp; if(x &gt; 0) \ &amp;kx, &amp;if(x &lt;&#x3D; 0) \end{aligned} \right. $$   &#x3D; max(x, kx) </p><p>其中k是一个很小的数，例如0.1或0.01等</p><ul><li><p>扩大了Relu值的范围，负无穷到正无穷</p></li><li><p>解决了Relu导致参数无法更新的问题，当输入为负数时，梯度为k</p></li></ul><p>尽管Leaky ReLU具备 ReLU 激活函数的所有特征（如计算高效、快速收敛、在正区域内不会饱和），但并不能完全证明在实际操作中Leaky ReLU 总是比 ReLU 更好。</p><h3 id="PRelu激活函数"><a href="#PRelu激活函数" class="headerlink" title="PRelu激活函数"></a>PRelu激活函数</h3><img src="NLP八股文.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5qCL5qyh5aSn5qyh,size_20,color_FFFFFF,t_70,g_se,x_16-20240523224253522.png" alt="lelu" style="zoom:50%;" /><p>PRelu和Leaky Relu类似，但是当x &lt; 0 时的导数k是可学习的（初始值为0.25）</p><h3 id="ELU激活函数"><a href="#ELU激活函数" class="headerlink" title="ELU激活函数"></a>ELU激活函数</h3><p>$$ f(x)&#x3D;\left{ \begin{aligned} &amp;x, &amp; if(x &gt; 0) \ &amp; \alpha(e^x - 1), &amp;if(x &lt;&#x3D; 0) \end{aligned} \right. $$ </p><p>$\alpha$是超参数，默认为1</p><p>缺点：计算量大，原点不可导</p><h3 id="GLU激活函数"><a href="#GLU激活函数" class="headerlink" title="GLU激活函数"></a>GLU激活函数</h3><p>![截屏2024-05-23 22.50.47](NLP八股文.assets&#x2F;截屏2024-05-23 22.50.47.png)</p><p>GLU其实是一个神经网络，其中 𝜎 为sigmoid函数，⊗ 为逐元素乘</p><h3 id="Gelu激活函数"><a href="#Gelu激活函数" class="headerlink" title="Gelu激活函数"></a>Gelu激活函数</h3><p>ReLU的平滑版本。</p><p>Gelu(x) &#x3D; x * norm.cdf(x)</p><h3 id="Swish激活函数"><a href="#Swish激活函数" class="headerlink" title="Swish激活函数"></a>Swish激活函数</h3><p>Relu的平滑版本</p><p>Swish(x) &#x3D; x * (1 &#x2F; (1 + exp(-$\beta $ x)))</p><img src="NLP八股文.assets/v2-0d644ecd419bbdfdae9be5a8c076b9f7_1440w.webp" alt="img" style="zoom:50%;" /><h3 id="SwiGLU激活函数"><a href="#SwiGLU激活函数" class="headerlink" title="SwiGLU激活函数"></a>SwiGLU激活函数</h3><p>![截屏2024-05-23 22.51.42](NLP八股文.assets&#x2F;截屏2024-05-23 22.51.42.png)</p><p>SwiGLU就是使用Swish作为激活函数的GLU变体</p><h2 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5.损失函数"></a>5.损失函数</h2><p>损失函数就是用来度量模型的预测值f(x)与真实值Y的差异程度的运算函数，它是一个非负实值函数。</p><p>损失函数使用主要是在模型的训练阶段，每个批次的训练数据送入模型后，通过前向传播输出预测值，然后损失函数会计算出预测值和真实值之间的差异值，也就是损失值。得到损失值之后，模型通过反向传播去更新各个参数，来降低真实值与预测值之间的损失，使得模型生成的预测值往真实值方向靠拢，从而达到学习的目的。</p><h3 id="MSE损失函数"><a href="#MSE损失函数" class="headerlink" title="MSE损失函数"></a>MSE损失函数</h3><p>MSE(y|f(x)) &#x3D; $\frac{1}{n}$ $\sum_{i&#x3D;1}^n$ (y<del>i</del> - f(x<del>i</del>))^2^</p><p>在回归问题中，均方误差损失函数用于度量样本点到回归曲线的距离，通过最小化平方损失使样本点可以更好地拟合回归曲线。</p><h3 id="L2损失函数"><a href="#L2损失函数" class="headerlink" title="L2损失函数"></a>L2损失函数</h3><p>L2(y|f(x)) &#x3D; $\sqrt{\frac{1}{n} \sum_{i&#x3D;1}^n (y_i - f(x_i))^2}$</p><h3 id="L1损失函数"><a href="#L1损失函数" class="headerlink" title="L1损失函数"></a>L1损失函数</h3><p>L1(y|f(x)) &#x3D; $\sum_{i&#x3D;1}^{n} |y_i - f(x_i)|$</p><h3 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h3><p>L(y|f(x)) &#x3D; $-\sum_{i&#x3D;1}^{n} y_ilog(f(x_i))|$</p><p>用于评估当前训练得到的概率分布与真实分布的差异情况</p><p>交叉熵损失函数刻画了实际输出概率与期望输出概率之间的相似度，也就是交叉熵的值越小，两个概率分布就越接近</p><h3 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h3><p>KL(y|f(x)) &#x3D; $\sum_{i&#x3D;1}^{n} y_ilog(\frac{y_i}{f(x_i)})|$</p><p>KL(P||Q) &#x3D; $\sum{p(x)\frac{p(x)}{q(x)}}$</p><p>KL散度也被称为相对熵，是一种非对称度量方法，常用于度量两个概率分布之间的距离。</p><p>KL散度也可以衡量两个随机分布之间的距离，两个随机分布的相似度越高的，它们的KL散度越小，当两个随机分布的差别增大时，它们的KL散度也会增大</p><p>KL散度是恒大于等于0的。当且仅当两分布相同时，相对熵等于0。</p><h3 id="JS散度"><a href="#JS散度" class="headerlink" title="JS散度"></a>JS散度</h3><p>用于衡量两个概率分布之间的相似度，它是基于KL散度的一种变形，消除了KL散度非对称的问题，与KL散度相比，它使得相似度判别更加准确。</p><p>M &#x3D; (P + Q) &#x2F; 2</p><p>JS(P||Q) &#x3D; 1&#x2F;2 KL(P||M) + 1&#x2F;2 KL(Q||M)</p><p>当两个分布完全不重合时，JS散度是一个常数（log2），梯度为0</p><h2 id="6-优化器"><a href="#6-优化器" class="headerlink" title="6.优化器"></a>6.优化器</h2><h2 id="7-正则化"><a href="#7-正则化" class="headerlink" title="7.正则化"></a>7.正则化</h2><h3 id="7-1-BatchNorm和LayerNorm的区别"><a href="#7-1-BatchNorm和LayerNorm的区别" class="headerlink" title="7.1 BatchNorm和LayerNorm的区别"></a>7.1 BatchNorm和LayerNorm的区别</h3><p>Norm的目的是为了将数据拉回标准正态分布，让训练更稳定。</p><p>BatchNorm是拿一个Batch的每一维特征进行归一化，LayerNorm是拿每个样本的所有特征进行归一化。</p><p>BN抹杀了不同特征之间的大小关系（将Batch内每一维特征进行归一化），保留了不同样本间的大小关系</p><p>LN抹杀了不同样本之间的大小关系（将每个样本的所有特征进行归一化），保留了不同特征间的大小关系。</p><p>因为每个序列的长度通常不一样（虽然有padding，但无意义），所以无法做batchnorm。</p><h3 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h3><p>L2 &#x3D; $\sqrt{\frac{1}{n} \sum_{i&#x3D;1}^n f(x_i)^2}$</p><h3 id="L1范数"><a href="#L1范数" class="headerlink" title="L1范数"></a>L1范数</h3><p>L1 &#x3D; $\sum_{i&#x3D;1}^{n} |f(x)_i|$</p><h2 id="8-LLM"><a href="#8-LLM" class="headerlink" title="8. LLM"></a>8. LLM</h2><h3 id="Continue-Pretraining和SFT"><a href="#Continue-Pretraining和SFT" class="headerlink" title="Continue Pretraining和SFT"></a>Continue Pretraining和SFT</h3><p>Continue Pretraining是在领域数据上进行预训练，使用的是没有标签的数据，数据格式和预训练的一样。Continue Pretraining的目的是让LLM更好地处理和理解领域数据</p><p>SFT是使用特定任务的数据集，有标签。SFT的目的是优化LLM在特定任务上的表现。</p><h3 id="如何解决继续预训练和微调阶段的灾难性遗忘问题"><a href="#如何解决继续预训练和微调阶段的灾难性遗忘问题" class="headerlink" title="如何解决继续预训练和微调阶段的灾难性遗忘问题"></a>如何解决继续预训练和微调阶段的灾难性遗忘问题</h3><p>目前不少开源模型在通用领域具有不错的效果，但由于缺乏领域数据，往往在一些垂直领域中表现不理想，这时就需要<a href="https://www.zhihu.com/search?q=%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3373620546%7D">增量预训练</a>和微调等方法来提高模型的领域能力。</p><p>但在领域数据增量预训练或微调时，很容易出现灾难性遗忘现象，也就是学会了垂直领域知识，但忘记了通用领域知识。</p><p>解决方法：</p><ul><li>调整继续预训练时的数据配比，增加通用数据的比例，增加数据量</li><li>使用adapter微调，对于每个任务微调一个adapter</li><li>使用正则化的办法可以降低对预训练特征的破坏</li></ul><h3 id="LLM训练、推理如何省显存"><a href="#LLM训练、推理如何省显存" class="headerlink" title="LLM训练、推理如何省显存"></a>LLM训练、推理如何省显存</h3><p>训练</p><ul><li>使用fp16，bf16等低精度来进行训练</li><li>梯度检查点：删除中间变量，需要的时候再重新计算</li><li><strong>ZERO系列优化</strong>，参数&#x2F;梯度&#x2F;优化器状态等分片存储，存到内存中，即用即取</li><li>减小batch size，使用梯度累积</li></ul><p>推理：</p><p>- </p><h3 id="如何缓解大模型幻觉问题"><a href="#如何缓解大模型幻觉问题" class="headerlink" title="如何缓解大模型幻觉问题"></a>如何缓解大模型幻觉问题</h3><ul><li>高级提示词，带有详细说明的复杂提示词能够缓解幻觉问题</li><li>RAG，检索外部知识库</li><li>验证链：<ul><li>生成基线响应：给定一个查询，使用 LLM 生成响应。</li><li>设计验证：给定查询和基线响应，生成验证问题列表，有助于自我分析原始响应中是否存在任何错误。</li><li>执行验证：依次回答每个验证问题，然后将答案与原始响应进行检查，以检查是否存在不一致或错误。</li><li>生成最终验证响应：鉴于发现的不一致（如果有），生成包含验证结果的修订响应。</li></ul></li><li></li></ul>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>八股文</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
